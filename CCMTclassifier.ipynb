{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7650470,"sourceType":"datasetVersion","datasetId":4459907}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":3742.205418,"end_time":"2024-02-27T13:09:54.518385","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-27T12:07:32.312967","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils, datasets\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom sklearn.metrics import classification_report, confusion_matrix\ntorch.cuda.empty_cache()\nfrom PIL import Image\nimport warnings\nimport cv2","metadata":{"papermill":{"duration":7.986845,"end_time":"2024-02-27T12:07:42.903900","exception":false,"start_time":"2024-02-27T12:07:34.917055","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T12:43:57.097444Z","iopub.execute_input":"2024-02-29T12:43:57.098308Z","iopub.status.idle":"2024-02-29T12:44:05.254526Z","shell.execute_reply.started":"2024-02-29T12:43:57.098271Z","shell.execute_reply":"2024-02-29T12:44:05.253709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nImage.LOAD_TRUNCATED_IMAGES = True\n\ndef list_images_train(base_dir):\n    images_with_labels = []\n    for class_dir in os.listdir(base_dir):\n        for type_dir in ['train_set']:  \n            for subclass_dir in os.listdir(os.path.join(base_dir, class_dir, type_dir)):\n                images_dir = os.path.join(base_dir, class_dir, type_dir, subclass_dir)\n                for image in os.listdir(images_dir):\n                    if image.lower().endswith(('.png', '.jpg', '.jpeg')): \n                        image_path = os.path.join(images_dir, image)\n                        label = f\"{class_dir}_{subclass_dir}\"\n                        images_with_labels.append((image_path, label))\n    return images_with_labels\n\nimport os\n\ndef list_images_test(base_dir):\n    images_with_labels = []\n    for class_dir in os.listdir(base_dir):\n        for type_dir in ['test_set']:  # Only looking into 'test' directory\n            for subclass_dir in os.listdir(os.path.join(base_dir, class_dir, type_dir)):\n                images_dir = os.path.join(base_dir, class_dir, type_dir, subclass_dir)\n                for image in os.listdir(images_dir):\n                    if image.lower().endswith(('.png', '.jpg', '.jpeg')):  # Filter for image files\n                        image_path = os.path.join(images_dir, image)\n                        label = f\"{class_dir}_{subclass_dir}\"\n                        images_with_labels.append((image_path, label))\n    return images_with_labels\n\nprint(len(list_images_train(\"/kaggle/input/teamken-data/Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented\")))\nprint(len(list_images_test(\"/kaggle/input/teamken-data/Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented\")))","metadata":{"papermill":{"duration":0.013675,"end_time":"2024-02-27T12:07:42.921308","exception":false,"start_time":"2024-02-27T12:07:42.907633","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T12:44:05.256013Z","iopub.execute_input":"2024-02-29T12:44:05.256477Z","iopub.status.idle":"2024-02-29T12:44:21.205465Z","shell.execute_reply.started":"2024-02-29T12:44:05.256449Z","shell.execute_reply":"2024-02-29T12:44:21.204517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import UnidentifiedImageError, Image\nfrom torch.utils.data import random_split\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image, UnidentifiedImageError\nimport os\nimport re\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.transform = transform\n        self.image_paths, self.labels = self._validate_images(image_paths, labels)\n        unique_labels = sorted(set(self.labels))\n        self.label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n\n    def _validate_images(self, image_paths, labels):\n        valid_image_paths = []\n        valid_labels = []\n        for path, label in zip(image_paths, labels):\n            try:\n                with Image.open(path) as img:\n                    img.load()\n                valid_image_paths.append(path)\n                modified_label = re.sub(r'\\d+$', '', label).rstrip(\"_\") \n                valid_labels.append(modified_label)\n            except (UnidentifiedImageError, OSError) as e:\n                print(f\"Skipping problematic image: {path} due to error: {e}\")\n                continue\n        return valid_image_paths, valid_labels\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        label = self.labels[index]\n        # Ensure label is processed for lookup in label_to_index\n        processed_label = re.sub(r'\\d+$', '', label).rstrip(\"_\") # Process label again during lookup\n        label_index = self.label_to_index[processed_label]\n        return image, label_index\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n    transforms.CenterCrop(size=224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nbase_dir = '/kaggle/input/teamken-data/Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented'\ntrain_data = list_images_train(base_dir)\ntest_data = list_images_test(base_dir)    \n\ntrain_image_paths, train_labels = zip(*train_data)\ntest_image_paths, test_labels = zip(*test_data)     \n\ntrain_image_paths, train_labels = list(train_image_paths), list(train_labels)\ntest_image_paths, test_labels = list(test_image_paths), list(test_labels)\n\ntrain_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n    train_image_paths, train_labels, test_size=0.2, random_state=42\n)\n\n","metadata":{"papermill":{"duration":169.91321,"end_time":"2024-02-27T12:10:32.837751","exception":false,"start_time":"2024-02-27T12:07:42.924541","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T12:44:42.711040Z","iopub.execute_input":"2024-02-29T12:44:42.711421Z","iopub.status.idle":"2024-02-29T12:44:43.342656Z","shell.execute_reply.started":"2024-02-29T12:44:42.711392Z","shell.execute_reply":"2024-02-29T12:44:43.341654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(image_paths=train_image_paths, labels=train_labels, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T12:44:46.733241Z","iopub.execute_input":"2024-02-29T12:44:46.734085Z","iopub.status.idle":"2024-02-29T12:57:36.092500Z","shell.execute_reply.started":"2024-02-29T12:44:46.734037Z","shell.execute_reply":"2024-02-29T12:57:36.091692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = CustomDataset(image_paths=val_image_paths, labels=val_labels, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T12:57:36.093999Z","iopub.execute_input":"2024-02-29T12:57:36.094305Z","iopub.status.idle":"2024-02-29T13:00:49.982724Z","shell.execute_reply.started":"2024-02-29T12:57:36.094278Z","shell.execute_reply":"2024-02-29T13:00:49.981868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = CustomDataset(image_paths=test_image_paths, labels=test_labels, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T13:00:49.983929Z","iopub.execute_input":"2024-02-29T13:00:49.984242Z","iopub.status.idle":"2024-02-29T13:05:08.806944Z","shell.execute_reply.started":"2024-02-29T13:00:49.984214Z","shell.execute_reply":"2024-02-29T13:05:08.805963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\nvalid_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T13:05:08.808883Z","iopub.execute_input":"2024-02-29T13:05:08.809199Z","iopub.status.idle":"2024-02-29T13:05:08.814753Z","shell.execute_reply.started":"2024-02-29T13:05:08.809173Z","shell.execute_reply":"2024-02-29T13:05:08.813720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(train_dataset.label_to_index)\nn_classes1 = len(test_dataset.label_to_index)\n\nprint(f\"Total number of unique train classes: {n_classes}\")\nprint(f\"Total number of unique test classes: {n_classes1}\")\n\nprint(train_dataset.label_to_index)","metadata":{"papermill":{"duration":0.014395,"end_time":"2024-02-27T12:10:32.858772","exception":false,"start_time":"2024-02-27T12:10:32.844377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T13:05:08.815711Z","iopub.execute_input":"2024-02-29T13:05:08.815939Z","iopub.status.idle":"2024-02-29T13:05:08.830756Z","shell.execute_reply.started":"2024-02-29T13:05:08.815919Z","shell.execute_reply":"2024-02-29T13:05:08.829897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')","metadata":{"papermill":{"duration":0.063784,"end_time":"2024-02-27T12:10:32.928925","exception":false,"start_time":"2024-02-27T12:10:32.865141","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T13:12:15.282401Z","iopub.execute_input":"2024-02-29T13:12:15.283069Z","iopub.status.idle":"2024-02-29T13:12:15.341505Z","shell.execute_reply.started":"2024-02-29T13:12:15.283024Z","shell.execute_reply":"2024-02-29T13:12:15.340532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n\nmodel = models.resnet18(pretrained=True)\n\nfrom torch import optim\nn_classes=22\nn_inputs = model.fc.in_features\nmodel.fc = nn.Sequential(\n                      nn.Linear(n_inputs, 128),\n                      nn.ReLU(),\n                      nn.Dropout(0.6),\n                      nn.Linear(128, n_classes),                   \n                      nn.LogSoftmax(dim=1))\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=5e-5)\n\ntrain_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    model = model.to('cuda')","metadata":{"papermill":{"duration":2.776831,"end_time":"2024-02-27T12:10:35.712316","exception":false,"start_time":"2024-02-27T12:10:32.935485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T13:12:17.495442Z","iopub.execute_input":"2024-02-29T13:12:17.496261Z","iopub.status.idle":"2024-02-29T13:12:17.969893Z","shell.execute_reply.started":"2024-02-29T13:12:17.496223Z","shell.execute_reply":"2024-02-29T13:12:17.969061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef train(model,\n          criterion,\n          optimizer,\n          train_loader,\n          valid_loader,\n          save_file_name,\n          max_epochs_stop=3,\n          n_epochs=20,\n          print_every=1):\n    \"\"\"Train a PyTorch Model\n    Params\n    --------\n        model (PyTorch model): cnn to train\n        criterion (PyTorch loss): objective to minimize\n        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n        train_loader (PyTorch dataloader): training dataloader to iterate through\n        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n        save_file_name (str ending in '.pt'): file path to save the model state dict\n        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n        n_epochs (int): maximum number of training epochs\n        print_every (int): frequency of epochs to print training stats\n    Returns\n    --------\n        model (PyTorch model): trained cnn with best weights\n        history (DataFrame): history of train and validation loss and accuracy\n    \"\"\"\n\n    epochs_no_improve = 0\n    valid_loss_min = np.Inf\n\n    valid_max_acc = 0\n    history = []\n\n    try:\n        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n    except:\n        model.epochs = 0\n        print(f'Starting Training from Scratch.\\n')\n\n    overall_start = time.time()\n\n    for epoch in range(n_epochs):\n\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        model.train()\n        start = time.time()\n\n        for ii, (data, target) in enumerate(train_loader):\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.',\n                end='\\r')\n        else:\n            model.epochs += 1\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_loader:\n                    if train_on_gpu:\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                train_loss = train_loss / len(train_loader.dataset)\n                valid_loss = valid_loss / len(valid_loader.dataset)\n                train_acc = train_acc / len(train_loader.dataset)\n                valid_acc = valid_acc / len(valid_loader.dataset)\n                history.append([train_loss, valid_loss, train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    print(\n                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n                    )\n                    print(\n                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n                    )\n                if valid_loss < valid_loss_min:\n                    torch.save(model.state_dict(), save_file_name)\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    valid_best_acc = valid_acc\n                    best_epoch = epoch\n                else:\n                    epochs_no_improve += 1\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(\n                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n                        )\n                        total_time = time.time() - overall_start\n                        print(\n                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n                        )\n                        model.load_state_dict(torch.load(save_file_name))\n                        model.optimizer = optimizer\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history\n\n    model.optimizer = optimizer\n    total_time = time.time() - overall_start\n\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n    return model, history\n\nmodel, history = train(\n    model,\n    criterion,\n    optimizer,\n    train_loader,\n    valid_loader,\n    save_file_name='/kaggle/working/model_checkpoint.pth',\n    max_epochs_stop=7,\n    n_epochs=100,\n    print_every=1)","metadata":{"papermill":{"duration":3533.14075,"end_time":"2024-02-27T13:09:28.861565","exception":false,"start_time":"2024-02-27T12:10:35.720815","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T13:12:27.301036Z","iopub.execute_input":"2024-02-29T13:12:27.301396Z","iopub.status.idle":"2024-02-29T13:21:08.381841Z","shell.execute_reply.started":"2024-02-29T13:12:27.301369Z","shell.execute_reply":"2024-02-29T13:21:08.380418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\nimport torch\n\ndef test(model, criterion, test_loader, train_on_gpu=False):\n    test_loss = 0.0\n    test_acc = 0.0\n    y_true = []\n    y_pred = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            output = model(data)\n            loss = criterion(output, target)\n            test_loss += loss.item() * data.size(0)\n\n            _, pred = torch.max(output, 1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            test_acc += accuracy.item() * data.size(0)\n\n            y_true.extend(target.cpu().numpy())\n            y_pred.extend(pred.cpu().numpy())\n\n    test_loss = test_loss / len(test_loader.dataset)\n    test_acc = test_acc / len(test_loader.dataset)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    acc = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    print(f'\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc * 100:.2f}%')\n    print(f'F1 Score: {f1:.4f} | Accuracy: {acc * 100:.2f}%')\n    print(f'Precision: {precision:.4f} | Recall: {recall:.4f}')\n\ntest(model, criterion, test_loader, train_on_gpu)\n","metadata":{"papermill":{"duration":21.563878,"end_time":"2024-02-27T13:09:51.186611","exception":false,"start_time":"2024-02-27T13:09:29.622733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-29T13:21:22.855018Z","iopub.execute_input":"2024-02-29T13:21:22.855423Z","iopub.status.idle":"2024-02-29T13:23:49.057984Z","shell.execute_reply.started":"2024-02-29T13:21:22.855393Z","shell.execute_reply":"2024-02-29T13:23:49.056856Z"},"trusted":true},"execution_count":null,"outputs":[]}]}